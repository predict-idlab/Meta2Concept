{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read glossary\n",
    "glossary = pd.read_csv('../data/metadata2kg/round2/r2_glossary_processed.csv')\n",
    "# read sample metadata\n",
    "metadata = pd.read_json('../data/metadata2kg/round2/r2_test_metadata.jsonl', lines=True)\n",
    "# concat index number with id to make it unique\n",
    "metadata['id'] = metadata.index.astype(str) + '_' + metadata['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Given a table '{table_name}' with columns:\n",
    "\n",
    "{col_descriptions}\n",
    "\n",
    "Your task is to determine which of the above is the best description for the column '{label}' given the context of the other columns.\n",
    "I will provide you with {num} descriptions, each indicated by number identifier [].\n",
    "\n",
    "{descriptions}\n",
    "You must give only the best description and provide the unique identifier using the output format [].\n",
    "\n",
    "### Example\n",
    "Analysis: <your step-by-step reasoning here>\n",
    "\n",
    "[identifier of the best fitting description]\n",
    "\n",
    "### Answer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank = read_jsonl_file('mapping_completion.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import time\n",
    "\n",
    "def background(f):\n",
    "    def wrapped(*args, **kwargs):\n",
    "        return asyncio.get_event_loop().run_in_executor(None, f, *args, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "@background\n",
    "def fn(chunk):\n",
    "    for i, table in chunk.iterrows():\n",
    "      json_file = 'test_completion_table_gpt_sc0/output' + str(i) + '.json'\n",
    "\n",
    "      # convert string to seed offset\n",
    "      seed_offset = sum([ord(c) for c in json_file]) \n",
    "\n",
    "      # if directory does not exist, create it\n",
    "      if not os.path.exists(json_file[:json_file.rfind('/')]):\n",
    "        os.makedirs(json_file[:json_file.rfind('/')])\n",
    "\n",
    "      if os.path.isfile(json_file):\n",
    "        continue\n",
    "      id = '_'.join(table['id'].split('_')[1:])\n",
    "      maps = [m['mappings'] for m in rerank if m['id'] == id][-1]\n",
    "      top_k = [int(m['id']) for m in maps][:5]\n",
    "\n",
    "      document = glossary[glossary['id'].isin(top_k)]\n",
    "      print(len(document['id'].tolist()), document['id_no_prefix'].tolist())\n",
    "\n",
    "      print('Table: ', i, table['label'])\n",
    "\n",
    "      for retry in range(10):\n",
    "        try:          \n",
    "            # second prompt\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": prompt.format(table_name=table['table_name'],\n",
    "                                                        label=table['label'],\n",
    "                                                        descriptions=print_descriptions((document['label'] + \", \" +document['desc']).tolist()),\n",
    "                                                        num = len(document['desc'].tolist()),\n",
    "                                                        col_descriptions='\\n'.join(get_column_descriptions(table, glossary, rerank))\n",
    "                                                        )}\n",
    "            ]\n",
    "            print(messages[0]['content'])\n",
    "            m2 = message_gpt(messages, temperature=0.0, seed=retry+seed_offset)\n",
    "            print(m2)\n",
    "\n",
    "            # write output to json file\n",
    "            with open(json_file, 'w') as f:\n",
    "                # write json object\n",
    "                f.write(json.dumps({\n",
    "                    'table': table.to_json(),\n",
    "                    'document': document['id_no_prefix'].tolist(),\n",
    "                    'analysis_1_match': m2,\n",
    "                    'k_matches': document['id_no_prefix'].tolist(),\n",
    "                    '1_match': [document.iloc[extract_identifiers(m2)[-1]]['id_no_prefix']],\n",
    "                }, indent=4))\n",
    "               \n",
    "            # for now never retry\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "        print('Retrying: ', json_file)\n",
    "        time.sleep(2)\n",
    "\n",
    "n_clients = 20\n",
    "\n",
    "for i in range(n_clients):\n",
    "    fn(metadata[metadata.index%n_clients==i])\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self consistency join\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "folders = ['test_completion_table_gpt_sc0']\n",
    "output_folder = 'test_completion_table'\n",
    "\n",
    "# if directory exists, its contents\n",
    "if os.path.exists(output_folder):\n",
    "    for file in glob.glob(output_folder + '/*.json'):\n",
    "        os.remove(file)\n",
    "else:\n",
    "    os.makedirs(output_folder)\n",
    "            \n",
    "for folder in folders:\n",
    "    for file in glob.glob(folder + '/*.json'):\n",
    "        with open(file, 'r') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(file)\n",
    "                continue\n",
    "\n",
    "        # if file does not exist, write it and copy\n",
    "        if not os.path.isfile(output_folder + '/' + os.path.basename(file)):\n",
    "            # write output to json file\n",
    "            with open(output_folder + '/' + os.path.basename(file), 'w') as f:\n",
    "                # convert k_matches to [{id: id, count: count}]\n",
    "                data['n_samples'] = 1\n",
    "                data['k-matches'] = [{'id': id, 'count': 1, 'rrf': 1/(rank+0.01)} for rank, id in enumerate(data['k_matches'])]\n",
    "                data['1_match'] = [{'id': id, 'count': 1, 'rrf': 1/(rank+0.01)} for rank, id in enumerate(data['1_match'])]\n",
    "                # write json object\n",
    "                f.write(json.dumps(data, indent=4))\n",
    "        # count matches\n",
    "        else:\n",
    "            with open(output_folder + '/' + os.path.basename(file), 'r') as f:\n",
    "                data2 = json.load(f)\n",
    "\n",
    "            # if k_matches in k-matches, increment count else add to k-matches\n",
    "            for rank, id in enumerate(data['k_matches']):\n",
    "                for d in data2['k-matches']:\n",
    "                    if id == d['id']:\n",
    "                        d['count'] += 1\n",
    "                        d['rrf'] += 1/(rank+0.01)\n",
    "                        break\n",
    "                else:\n",
    "                    data2['k-matches'].append({'id': id, 'count': 1, 'rrf': 1/(rank+0.01)})\n",
    "\n",
    "            # if 1_match in 1_match, increment count else add to 1_match\n",
    "            for rank, id in enumerate(data['1_match']):\n",
    "                for d in data2['1_match']:\n",
    "                    if id == d['id']:\n",
    "                        d['count'] += 1\n",
    "                        d['rrf'] += 1/(rank+0.01)\n",
    "                        break\n",
    "                else:\n",
    "                    data2['1_match'].append({'id': id, 'count': 1, 'rrf': 1/(rank+0.01)})\n",
    "\n",
    "            # sort by count\n",
    "            data2['n_samples'] = data2['n_samples'] + 1\n",
    "            data2['k-matches'] = sorted(data2['k-matches'], key=lambda x: x['rrf'], reverse=True)\n",
    "            data2['1_match'] = sorted(data2['1_match'], key=lambda x: x['rrf'], reverse=True)\n",
    "\n",
    "            # drop\n",
    "            data2.pop('analysis_k_matches', None)\n",
    "            data2.pop('analysis_1_match', None)\n",
    "\n",
    "            # write output to json file\n",
    "            with open(output_folder + '/' + os.path.basename(file), 'w') as f:\n",
    "                # write json object\n",
    "                f.write(json.dumps(data2, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "folder = 'test_completion_table'\n",
    "\n",
    "files = glob.glob(folder + '/output*.json')\n",
    "\n",
    "# read all json files\n",
    "data = []\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        data.append(json.load(f))\n",
    "\n",
    "# create mapping file\n",
    "mapping = []\n",
    "for d in data:\n",
    "    try:\n",
    "        # parse table as json object and get id\n",
    "        table = json.loads(d['table'])\n",
    "        id = table['id']\n",
    "        mappings = []\n",
    "\n",
    "        for matches in d['k-matches']:\n",
    "            mappings.append({'id': matches['id'], 'score': matches['rrf']})\n",
    "\n",
    "        # add small value to put best match on top\n",
    "        best_match = d['1_match'][0]\n",
    "        for match in mappings:\n",
    "            if match['id'] == best_match['id']:\n",
    "                match['score'] += best_match['rrf']\n",
    "\n",
    "        # sort by score\n",
    "        mappings = sorted(mappings, key = lambda i: i['score'], reverse=True)\n",
    "\n",
    "        # round to 3 decimals\n",
    "        for match in mappings:\n",
    "            match['score'] = round(match['score'], 3)\n",
    "\n",
    "        mapping.append({'id': '_'.join(id.split('_')[1:]), 'mappings': mappings})\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# change id_no_prefix to id\n",
    "for table in mapping:\n",
    "    for match in table['mappings']:\n",
    "        match['id'] = str(glossary[glossary['id_no_prefix'] == match['id']]['id'].values[0])\n",
    "\n",
    "# write mapping file\n",
    "with open('mapping_completion_table.jsonl', 'w') as f:\n",
    "    for m in mapping:\n",
    "        f.write(json.dumps(m) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6648d82896bd203fd466e78bea17536dc66c69ff4a963dcb65bdd261657c162"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
