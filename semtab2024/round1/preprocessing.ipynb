{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read glossary\n",
    "glossary = pd.read_json('../data/metadata2kg/round1/r1_glossary_enriched.jsonl', lines=True)\n",
    "glossary['id_no_prefix'] = glossary['id'].str.replace('http://dbpedia.org/ontology/', '')\n",
    "# if id_no_prefix has duplicates, add a number to the end\n",
    "glossary['c'] = glossary.groupby('id_no_prefix').cumcount()\n",
    "glossary['c'] = glossary['c'].apply(lambda x: '' if x == 0 else str(x))\n",
    "glossary['id_no_prefix'] = glossary['id_no_prefix'] + glossary['c']\n",
    "glossary = glossary.drop(columns=['c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "glossary.to_csv('../data/metadata2kg/round1/r1_glossary_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary_prompt = \"\"\"\"Can you clean up the following? Just fix mistakes or translate to english if necessary. Also change obscure words and you must write abbreviations and acronyms in full. Convert obscure names to their type.\n",
    "### Example\n",
    "Q: \n",
    "0. last win\n",
    "1. charles\n",
    "2. Harelbeke\n",
    "\n",
    "A: \n",
    "0. Last win\n",
    "1. King (Charles)\n",
    "2. City (Harelbeke)\n",
    "\n",
    "### Data\n",
    "\n",
    "{data}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary_desc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "def background(f):\n",
    "    def wrapped(*args, **kwargs):\n",
    "        return asyncio.get_event_loop().run_in_executor(None, f, *args, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "@background\n",
    "def fn(groups):\n",
    "    for chunk in groups:\n",
    "        if len(set(chunk['id']).intersection(glossary_desc.keys())) > 0:\n",
    "            print('Already processed')\n",
    "            return\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": glossary_prompt.format(data=print_descriptions(chunk['label'].tolist()))},\n",
    "        ]\n",
    "\n",
    "        print(messages[0]['content'])\n",
    "        #print(messages[0]['content'])\n",
    "        m = message_gpt(messages, temperature=0.0, seed=47)\n",
    "\n",
    "        content = extract_content(m)\n",
    "\n",
    "        if len(content) != len(chunk):\n",
    "            print('Error: Length mismatch')\n",
    "            return\n",
    "        \n",
    "        for id, content in zip(chunk['id'], content):\n",
    "            glossary_desc[id] = content\n",
    "\n",
    "n_clients = 20\n",
    "\n",
    "# group metadata in chunks of 100\n",
    "grouped = glossary.groupby([i//100 for i in range(len(glossary))])\n",
    "\n",
    "for i in range(n_clients):\n",
    "    group_idx = np.arange(len(grouped))%n_clients==i\n",
    "    fn([grouped.get_group(i) for i in range(len(grouped)) if group_idx[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# save glossary descriptions\n",
    "with open('glossary_descriptions_cleaned.json', 'w') as f:\n",
    "    json.dump(glossary_desc, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata\n",
    "metadata = pd.read_json('../data/metadata2kg/round1/r1_test_metadata.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_prompt = \"\"\"Can you clean up the following? Fix mistakes and put everything in english if not. Write abbreviations or acronyms in full and add the abbreviation in brackets after the word. Convert names to their type. Add a relation e.g. is a, of a or other. You must understand the meaning of the word first to write the relation.\n",
    "\n",
    "### Example\n",
    "Q: \n",
    "0. year, film\n",
    "1. developer, videogame\n",
    "2. harelbeke, country\n",
    "\n",
    "A: \n",
    "0. Year of a film\n",
    "1. Developer of a videogame\n",
    "3. City (Harelbeke) of a country\n",
    "\n",
    "### Data\n",
    "\n",
    "{data}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_desc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": metadata_prompt.format(data=print_descriptions(\n",
    "        metadata.apply(lambda x: '{label}, {table_name}'.format(label=x['label'], table_name=x['table_name'].lower()), axis=1)))}\n",
    "]\n",
    "\n",
    "m = message_gpt(messages, temperature=0.0, seed=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metadata['emb_desc'] = extract_content(m)\n",
    "metadata_desc = metadata.set_index('id')['emb_desc'].to_dict()\n",
    "\n",
    "with open('metadata_descriptions_cleaned.json', 'w') as f:\n",
    "    json.dump(metadata_desc, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
