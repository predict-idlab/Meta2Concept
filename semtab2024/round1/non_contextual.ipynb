{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read glossary\n",
    "glossary = pd.read_csv('../data/metadata2kg/round1/r1_glossary_processed.csv')\n",
    "# convert domain, range columns to str\n",
    "glossary['domain'] = glossary['domain'].astype(str)\n",
    "glossary['range'] = glossary['range'].astype(str)\n",
    "# replace 'nan' with ''\n",
    "glossary['domain'] = glossary['domain'].replace('nan', '')\n",
    "glossary['range'] = glossary['range'].replace('nan', '')\n",
    "# read sample metadata\n",
    "metadata = pd.read_json('../data/metadata2kg/round1/r1_test_metadata.jsonl', lines=True)\n",
    "# concat index number with id to make it unique\n",
    "#metadata['id'] = metadata.index.astype(str) + '_' + metadata['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are given a table in JSON-LD standard and a prompt. Take into account the table_name and table_columns to rank the top five official fine-grained property from dbpedia.org/ontology/ vocabulary for the prompt column. Don't respond in json.\n",
    "\n",
    "Example:\n",
    "table: \n",
    "{{\"table_name\": \"Museum\", \"table_columns\": [\"RANG\", \"Museum\", \"Stadt\", \"Facebook-Fans\"]}}\n",
    "\n",
    "prompt: Stadt\n",
    "\n",
    "fine-grained property:\n",
    "1: location\n",
    "2: locationCity\n",
    "3: locationName\n",
    "4: city\n",
    "5: livingPlace\n",
    "\n",
    "Task:\n",
    "table: \n",
    "{{\"table_name\": \"{table_name}\", \"table_columns\": {col_names}}}\n",
    "\n",
    "prompt: {label}\n",
    "\n",
    "fine-grained property:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import time\n",
    "\n",
    "def background(f):\n",
    "    def wrapped(*args, **kwargs):\n",
    "        return asyncio.get_event_loop().run_in_executor(None, f, *args, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "@background\n",
    "def fn(chunk):\n",
    "    for i, table in chunk.iterrows():\n",
    "      json_file = 'test_completion_baseline_k_matches_gpt_sc0/output' + str(i) + '.json' # change for each run\n",
    "\n",
    "      # convert string to seed offset\n",
    "      seed_offset = sum([ord(c) for c in json_file]) \n",
    "\n",
    "      # if directory does not exist, create it\n",
    "      if not os.path.exists(json_file[:json_file.rfind('/')]):\n",
    "        os.makedirs(json_file[:json_file.rfind('/')])\n",
    "\n",
    "      if os.path.isfile(json_file):\n",
    "        continue\n",
    "      \n",
    "      id = table['id']\n",
    "      print(id)\n",
    "      print('Table: ', i, table['label'])\n",
    "\n",
    "      for retry in range(10):\n",
    "        try:          \n",
    "            # second prompt\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": prompt.format(table_name=table['table_name'],\n",
    "                                                        label=table['label'],\n",
    "                                                        col_names=table['table_columns'])}\n",
    "            ]\n",
    "            #print(messages[0]['content'])\n",
    "            m2 = message_gpt(messages, temperature=0.7, seed=retry+seed_offset)\n",
    "            print(m2)\n",
    "            print(get_numbered_ids(m2))\n",
    "\n",
    "            # write output to json file\n",
    "            with open(json_file, 'w') as f:\n",
    "                # write json object\n",
    "                f.write(json.dumps({\n",
    "                    'table': table.to_json(),\n",
    "                    'document': [],\n",
    "                    'analysis_k_matches': m2,\n",
    "                    'k_matches': get_numbered_ids(m2)[-10:],\n",
    "                    '1_match': []\n",
    "                }, indent=4))\n",
    "               \n",
    "            # for now never retry\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e),\n",
    "            pass\n",
    "\n",
    "        print('Retrying: ', json_file)\n",
    "        time.sleep(2)\n",
    "\n",
    "n_clients = 20\n",
    "\n",
    "for i in range(n_clients):\n",
    "    fn(metadata[metadata.index%n_clients==i])\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self consistency join\n",
    "import glob\n",
    "\n",
    "folders = [] # add folders here\n",
    "output_folder = 'test_completion_baseline_k_matches'\n",
    "\n",
    "# if directory exists, its contents\n",
    "if os.path.exists(output_folder):\n",
    "    for file in glob.glob(output_folder + '/*.json'):\n",
    "        os.remove(file)\n",
    "else:\n",
    "    os.makedirs(output_folder)\n",
    "            \n",
    "for folder in folders:\n",
    "    for file in glob.glob(folder + '/*.json'):\n",
    "        with open(file, 'r') as f:   \n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "        # if file does not exist, write it and copy\n",
    "        if not os.path.isfile(output_folder + '/' + os.path.basename(file)):\n",
    "            # write output to json file\n",
    "            with open(output_folder + '/' + os.path.basename(file), 'w') as f:\n",
    "                # convert k_matches to [{id: id, count: count}]\n",
    "                data['n_samples'] = 1\n",
    "                data['k-matches'] = [{'id': id, 'count': 1, 'rrf': 1/(rank+0.01)} for rank, id in enumerate(data['k_matches'])]\n",
    "                # write json object\n",
    "                f.write(json.dumps(data, indent=4))\n",
    "\n",
    "        # count matches\n",
    "        else:\n",
    "            with open(output_folder + '/' + os.path.basename(file), 'r') as f:\n",
    "                data2 = json.load(f)\n",
    "\n",
    "            # if k_matches in k-matches, increment count else add to k-matches\n",
    "            for rank, id in enumerate(data['k_matches']):\n",
    "                for d in data2['k-matches']:\n",
    "                    if id == d['id']:\n",
    "                        d['count'] += 1\n",
    "                        d['rrf'] += 1/(rank+0.01)\n",
    "                        break\n",
    "                else:\n",
    "                    data2['k-matches'].append({'id': id, 'count': 1, 'rrf': 1/(rank+0.01)})\n",
    "\n",
    "            # sort by count\n",
    "            data2['n_samples'] = data2['n_samples'] + 1\n",
    "            data2['k-matches'] = sorted(data2['k-matches'], key=lambda x: x['rrf'], reverse=True)\n",
    "\n",
    "            # drop\n",
    "            data2.pop('analysis_k_matches', None)\n",
    "            data2.pop('analysis_1_match', None)\n",
    "            # write output to json file\n",
    "            with open(output_folder + '/' + os.path.basename(file), 'w') as f:\n",
    "                # write json object\n",
    "                f.write(json.dumps(data2, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter ids that are not in the glossary\n",
    "for file in glob.glob(output_folder + '/*.json'):\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # filter k-matches\n",
    "    data['k-matches'] = [d for d in data['k-matches'] if d['id'] in glossary['id_no_prefix'].values]\n",
    "    data['1_match'] = [d for d in data['1_match'] if d['id'] in glossary['id_no_prefix'].values]\n",
    "\n",
    "    # write output to json file\n",
    "    with open(file, 'w') as f:\n",
    "        # write json object\n",
    "        f.write(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "folder = 'test_completion_baseline_k_matches'\n",
    "\n",
    "files = glob.glob(folder + '/output*.json')\n",
    "\n",
    "# read all json files\n",
    "data = []\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        data.append(json.load(f))\n",
    "\n",
    "# create mapping file\n",
    "mapping = []\n",
    "for d in data:\n",
    "    try:\n",
    "        # parse table as json object and get id\n",
    "        table = json.loads(d['table'])\n",
    "        id = table['id']\n",
    "        mappings = []\n",
    "\n",
    "        for matches in d['k-matches']:\n",
    "            mappings.append({'id': matches['id'], 'score': matches['rrf']})\n",
    "\n",
    "        # sort by score\n",
    "        mappings = sorted(mappings, key = lambda i: i['score'], reverse=True)\n",
    "\n",
    "        # normalize scores, round to 3 decimals\n",
    "        for match in mappings:\n",
    "            #match['score'] = match['score'] / len(mappings)\n",
    "            match['score'] = round(match['score'], 3)\n",
    "\n",
    "        mapping.append({'id': id, 'mappings': mappings})\n",
    "    except Exception as e:\n",
    "        print(d)\n",
    "        print(e)\n",
    "\n",
    "# change id_no_prefix to id\n",
    "for table in mapping:\n",
    "    #print(len(table['mappings']))\n",
    "    for match in table['mappings']:\n",
    "        match['id'] = 'http://dbpedia.org/ontology/' + match['id']\n",
    "\n",
    "# write mapping file\n",
    "with open('mapping_completion_baseline.jsonl', 'w') as f:\n",
    "    for m in mapping:\n",
    "        f.write(json.dumps(m) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6648d82896bd203fd466e78bea17536dc66c69ff4a963dcb65bdd261657c162"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
