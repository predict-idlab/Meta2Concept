{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read glossary\n",
    "glossary = pd.read_csv('../data/metadata2kg/round1/r1_glossary_processed.csv')\n",
    "# convert domain, range columns to str\n",
    "glossary['domain'] = glossary['domain'].astype(str)\n",
    "glossary['range'] = glossary['range'].astype(str)\n",
    "# replace 'nan' with ''\n",
    "glossary['domain'] = glossary['domain'].replace('nan', '')\n",
    "glossary['range'] = glossary['range'].replace('nan', '')\n",
    "# read sample metadata\n",
    "metadata = pd.read_json('../data/metadata2kg/round1/r1_test_metadata.jsonl', lines=True)\n",
    "# concat index number with id to make it unique\n",
    "#metadata['id'] = metadata.index.astype(str) + '_' + metadata['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load glossary descriptions\n",
    "with open('glossary_descriptions_cleaned.json', 'r') as f:\n",
    "    glossary_desc = json.load(f)\n",
    "glossary['emb_desc'] = glossary['id'].map(glossary_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Given a table '{table_name}' with columns {col_names} what is the best description for the column '{label}'?\n",
    "I will provide you with {num} triples in the format [domain, property, range] with a description of the property, each indicated by number identifier [].\n",
    "\n",
    "{descriptions}\n",
    "You must rank the top five descriptions best matching the column and provide the 5 unique identifiers using the output format [].\n",
    "\n",
    "### Example\n",
    "Analysis: <reasoning>\n",
    "\n",
    "1. [id1]\n",
    "2. [id2]\n",
    "3. [id3]\n",
    "4. [id4]\n",
    "5. [id5]\n",
    "\n",
    "### Answer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank = read_jsonl_file('mapping_rerank.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import time\n",
    "\n",
    "def background(f):\n",
    "    def wrapped(*args, **kwargs):\n",
    "        return asyncio.get_event_loop().run_in_executor(None, f, *args, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "@background\n",
    "def fn(chunk):\n",
    "    for i, table in chunk.iterrows():\n",
    "      json_file = 'test_completion_k_matches_gpt_sc0/output' + str(i) + '.json' # change this for each run\n",
    "\n",
    "      # convert string to seed offset\n",
    "      seed_offset = sum([ord(c) for c in json_file]) \n",
    "\n",
    "      # if directory does not exist, create it\n",
    "      if not os.path.exists(json_file[:json_file.rfind('/')]):\n",
    "        os.makedirs(json_file[:json_file.rfind('/')])\n",
    "\n",
    "      if os.path.isfile(json_file):\n",
    "        continue\n",
    "      \n",
    "      id = table['id']\n",
    "      print(id)\n",
    "      maps = [m['mappings'] for m in rerank if m['id'] == id][-1]\n",
    "      top_k = [m['id'] for m in maps][:30]\n",
    "      \n",
    "      document = glossary[glossary['id'].isin(top_k)]\n",
    "      print(len(document['id'].tolist()), document['id_no_prefix'].tolist())\n",
    "\n",
    "      # random shuffle the document for 'lost in the middle' problem\n",
    "      document = document.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "      print('Table: ', i, table['label'])\n",
    "\n",
    "      for retry in range(10):\n",
    "        try:          \n",
    "            # second prompt\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": prompt.format(table_name=table['table_name'],\n",
    "                                                        label=table['label'],\n",
    "                                                        descriptions=print_descriptions(print_glossary(document).split('\\n')),\n",
    "                                                        num = len(document['desc'].tolist()),\n",
    "                                                        col_names=table['table_columns'])}\n",
    "            ]\n",
    "            #print(messages[0]['content'])\n",
    "            m2 = message_gpt(messages, temperature=0.7, seed=retry+seed_offset)\n",
    "            print(m2)\n",
    "\n",
    "            # write output to json file\n",
    "            with open(json_file, 'w') as f:\n",
    "                # write json object\n",
    "                f.write(json.dumps({\n",
    "                    'table': table.to_json(),\n",
    "                    'document': document['id_no_prefix'].tolist(),\n",
    "                    'analysis_k_matches': m2,\n",
    "                    'k_matches': document.iloc[extract_identifiers(m2)[-5:]]['id_no_prefix'].tolist(),\n",
    "                    '1_match': []\n",
    "                }, indent=4))\n",
    "               \n",
    "            # for now never retry\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e),\n",
    "            pass\n",
    "\n",
    "        print('Retrying: ', json_file)\n",
    "        time.sleep(2)\n",
    "\n",
    "n_clients = 10\n",
    "\n",
    "for i in range(n_clients):\n",
    "    fn(metadata[metadata.index%n_clients==i])\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self consistency join\n",
    "import glob\n",
    "import os\n",
    "\n",
    "folders = [] # add folders here\n",
    "output_folder = 'test_completion_k_matches'\n",
    "\n",
    "# if directory exists, its contents\n",
    "if os.path.exists(output_folder):\n",
    "    for file in glob.glob(output_folder + '/*.json'):\n",
    "        os.remove(file)\n",
    "else:\n",
    "    os.makedirs(output_folder)\n",
    "            \n",
    "for folder in folders:\n",
    "    for file in glob.glob(folder + '/*.json'):\n",
    "        with open(file, 'r') as f:   \n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except:\n",
    "                print(file)\n",
    "                continue\n",
    "\n",
    "        # if file does not exist, write it and copy\n",
    "        if not os.path.isfile(output_folder + '/' + os.path.basename(file)):\n",
    "            # write output to json file\n",
    "            with open(output_folder + '/' + os.path.basename(file), 'w') as f:\n",
    "                # convert k_matches to [{id: id, count: count}]\n",
    "                data['n_samples'] = 1\n",
    "                data['k-matches'] = [{'id': id, 'count': 1, 'rrf': 1/(rank+0.01)} for rank, id in enumerate(data['k_matches'])]\n",
    "                # write json object\n",
    "                f.write(json.dumps(data, indent=4))\n",
    "\n",
    "        # count matches\n",
    "        else:\n",
    "            with open(output_folder + '/' + os.path.basename(file), 'r') as f:\n",
    "                data2 = json.load(f)\n",
    "\n",
    "            # if k_matches in k-matches, increment count else add to k-matches\n",
    "            for rank, id in enumerate(data['k_matches']):\n",
    "                for d in data2['k-matches']:\n",
    "                    if id == d['id']:\n",
    "                        d['count'] += 1\n",
    "                        d['rrf'] += 1/(rank+0.01)\n",
    "                        break\n",
    "                else:\n",
    "                    data2['k-matches'].append({'id': id, 'count': 1, 'rrf': 1/(rank+0.01)})\n",
    "\n",
    "            # if 1_match in 1_match, increment count else add to 1_match\n",
    "            for id in data['1_match']:\n",
    "                for d in data2['1_match']:\n",
    "                    if id == d['id']:\n",
    "                        d['count'] += 1\n",
    "                        break\n",
    "                else:\n",
    "                    data2['1_match'].append({'id': id, 'count': 1})\n",
    "\n",
    "            # sort by count\n",
    "            data2['n_samples'] = data2['n_samples'] + 1\n",
    "            data2['k-matches'] = sorted(data2['k-matches'], key=lambda x: x['rrf'], reverse=True)\n",
    "\n",
    "            # drop\n",
    "            data2.pop('analysis_k_matches', None)\n",
    "            data2.pop('analysis_1_match', None)\n",
    "\n",
    "            # write output to json file\n",
    "            with open(output_folder + '/' + os.path.basename(file), 'w') as f:\n",
    "                # write json object\n",
    "                f.write(json.dumps(data2, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "folder = 'test_completion_k_matches'\n",
    "\n",
    "files = glob.glob(folder + '/output*.json')\n",
    "\n",
    "# read all json files\n",
    "data = []\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        data.append(json.load(f))\n",
    "\n",
    "# create mapping file\n",
    "mapping = []\n",
    "for d in data:\n",
    "    try:\n",
    "        # parse table as json object and get id\n",
    "        table = json.loads(d['table'])\n",
    "        id = table['id']\n",
    "        mappings = []\n",
    "\n",
    "        for matches in d['k-matches']:\n",
    "            mappings.append({'id': matches['id'], 'score': matches['rrf']})\n",
    "\n",
    "        # sort by score\n",
    "        mappings = sorted(mappings, key = lambda i: i['score'], reverse=True)\n",
    "\n",
    "        # round to 3 decimals\n",
    "        for match in mappings:\n",
    "            match['score'] = round(match['score'], 3)\n",
    "\n",
    "        mapping.append({'id': id, 'mappings': mappings})\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# change id_no_prefix to id\n",
    "for table in mapping:\n",
    "    for match in table['mappings']:\n",
    "        match['id'] = str(glossary[glossary['id_no_prefix'] == match['id']]['id'].values[0])\n",
    "\n",
    "# write mapping file\n",
    "with open('mapping_completion.jsonl', 'w') as f:\n",
    "    for m in mapping:\n",
    "        f.write(json.dumps(m) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6648d82896bd203fd466e78bea17536dc66c69ff4a963dcb65bdd261657c162"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
